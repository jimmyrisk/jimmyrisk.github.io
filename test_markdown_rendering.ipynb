{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b35e8ae",
   "metadata": {},
   "source": [
    "# RPS Quest - Markdown Rendering Test\n",
    "\n",
    "This notebook tests that the mathematical notation from the RPS Quest webpage renders correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a7075",
   "metadata": {},
   "source": [
    "## State Representation\n",
    "\n",
    "The bot policy operates on a Markovian state representation. Let $u_t$ and $b_t$ denote the human and bot cumulative scores at the start of round $t$. The state at time $t$ is\n",
    "\n",
    "$$\n",
    "  s_t = \\big(x_{t-1}, x_{t-2}, x_{t-3}, \\mathcal{H}_{t-1}, z_t\\big),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_{t-1}, x_{t-2}, x_{t-3}$ are the three most recent human moves,\n",
    "- $\\mathcal{H}_{t-1}$ aggregates historical statistics computed from rounds $1, \\ldots, t-1$ (cumulative move frequencies, favored-move tendencies, lagged point values),\n",
    "- $z_t = (u_t, b_t, w_t, t)$ are the deterministic, known quantities at time $t$: current scores, round multipliers, and step index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a2191",
   "metadata": {},
   "source": [
    "## Round Multipliers\n",
    "\n",
    "Round multipliers are generated per game and round via hash:\n",
    "\n",
    "$$\n",
    "  w_t^{(\\ell)} = 1.0, \\qquad w_t^{(m)} = \\beta_t, \\qquad w_t^{(h)} = \\beta_t + 0.5,\n",
    "$$\n",
    "\n",
    "where $\\beta_t \\in \\{1.1, 1.2, 1.3, 1.4, 1.5\\}$ and $\\{\\ell, m, h\\}$ is a permutation of moves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea85312",
   "metadata": {},
   "source": [
    "## Model Output\n",
    "\n",
    "Given $\\phi(s_t)$, each model produces a categorical distribution over the next user move:\n",
    "\n",
    "$$\n",
    "  \\mathbf{p}_t = \\big(p_t^{(R)}, p_t^{(P)}, p_t^{(S)}\\big) = \\mathbb{P}(x_{t+1} \\mid s_t),\n",
    "$$\n",
    "\n",
    "which drives both the greedy policy and telemetry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb52b3",
   "metadata": {},
   "source": [
    "## HJB Equation\n",
    "\n",
    "The underlying control problem is a finite-horizon Markov decision process with a terminal condition at $\\tau = 10$ points. An optimal controller would solve the discrete-time Hamilton-Jacobi-Bellman (HJB) equation backward in time:\n",
    "\n",
    "$$\n",
    "  V_t(s) = \\max_{a \\in \\mathcal{A}} \\Big[ r_t(s,a) + \\gamma \\, \\mathbb{E}\\big[ V_{t+1}(S_{t+1}) \\mid s, a \\big] \\Big],\n",
    "$$\n",
    "\n",
    "with $V_T(s) = 0$ once either player hits the target score $\\tau$ and discount $\\gamma = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38316ec",
   "metadata": {},
   "source": [
    "## Greedy Q-Value Approximation\n",
    "\n",
    "For any candidate bot move $a$, let $b(a)$ denote the human action beaten by $a$ and $\\ell(a)$ the action that defeats $a$. The approximation computes:\n",
    "\n",
    "$$\n",
    "  \\widehat{Q}_t(a) = p_t^{(b(a))} \\Big(w_t^{(a)} + B_t(a)\\Big) - p_t^{(\\ell(a))} \\Big(w_t^{(\\ell(a))} + L_t(a)\\Big) + p_t^{(a)}\\, C_t,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "  B_t(a) = \\mathbf{1}\\{u_t + w_t^{(a)} \\ge \\tau\\}\\cdot \\tau, \\quad\n",
    "  L_t(a) = \\mathbf{1}\\{b_t + w_t^{(\\ell(a))} \\ge \\tau\\}\\cdot \\tau, \\quad\n",
    "  C_t = \\tfrac{1}{2}\\,\\frac{u_t - b_t}{\\tau}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97219723",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "âœ… All equations should render properly above with:\n",
    "- Greek letters ($\\beta_t$, $\\tau$, $\\gamma$)\n",
    "- Calligraphic fonts ($\\mathcal{H}_{t-1}$, $\\mathcal{A}$)\n",
    "- Blackboard bold ($\\mathbb{P}$, $\\mathbb{E}$)\n",
    "- Indicator functions ($\\mathbf{1}$)\n",
    "- Proper alignment in multi-line equations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
